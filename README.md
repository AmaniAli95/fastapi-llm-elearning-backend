# FastApi LLM Elearning Backend

This repository contains a FastAPI backend Retrieval Augmented Generation (RAG) application that leverages large language models (LLMs) like GPT-3.5 for question-answering capabilities.

## Overview
The backend provides RESTful APIs for:
- **User management**: account creation, authentication, profiles
- **Question answering**: submit a question, get back an answer generated by an LLM
- **Explanations**: ask follow-up questions, get explanations from the LLM
- **Conversations**: chat with the LLM in a conversational format
The core question answering and explanations are powered by OpenAI's GPT-3 API. The backend handles prompt formatting, calling the API, and processing the response.

## Features
- **User auth**: JWT-based authentication and user accounts
- **Question API**: Submit questions, receive AI-generated answers
- **Conversations API**: Chatbot-style conversations with the LLM
- **Explanations API**: Ask followup questions to get explanations
- **Moderation**: Optional filtering of generated text
- **Analytics**: Track usage metrics and query patterns

## Technologies
- **FastAPI**: High performance web framework
- **OpenAI API**: Large language models like GPT-3
- **SQLAlchemy**: Database ORM
- **LangChain**: Helper library for LLMs
- **Docker**: Containerization

## Getting Started

### Prerequisites
- Python 3.9+
- OpenAI API key
- Docker

### Installation
```bash
$ git clone https://github.com/llm-question-answering
$ cd backend
$ pip install -r requirements.txt
```

### Configuration
Add OpenAI API keys:

```python
OPENAI_API_KEY = 'sk-...' 
```

Running

```bash
docker-compose up --build
```
The FastAPI server will be available at **http://localhost:8000**
